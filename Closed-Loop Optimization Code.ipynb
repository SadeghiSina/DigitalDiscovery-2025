{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f12855",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "from scipy.stats import linregress\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "import re\n",
    "import torch\n",
    "import botorch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from botorch.models.model import Model\n",
    "from botorch.posteriors.posterior import Posterior\n",
    "from botorch.acquisition import LogExpectedImprovement, PosteriorMean\n",
    "from botorch.acquisition.analytic import LogProbabilityOfImprovement\n",
    "from botorch.generation.gen import gen_candidates_torch\n",
    "from botorch.optim.optimize import optimize_acqf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d432fc",
   "metadata": {},
   "source": [
    "### Real-Time Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d09c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural sorting \n",
    "def natural_sort_key(s):\n",
    "    _nsre = re.compile('([0-9]+)')\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)]  \n",
    "## set of function to read and process the raw spectra\n",
    "def read_files(loc):\n",
    "    files_all = sorted(os.listdir(loc), key = natural_sort_key)\n",
    "    ABS_names = []\n",
    "    PL_names = []\n",
    "    ABS_all = []\n",
    "    PL_all = []\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            ABS = pd.read_csv(loc + \"//\" + file , header = None).to_numpy()\n",
    "            ABS_all.append(ABS)\n",
    "            ABS_names.append(file.split(\".\")[0])\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "            PL_all.append(PL)\n",
    "            PL_names.append(file.split(\".\")[0])\n",
    "        elif(file.startswith(\"Wavelength_Abs\")):\n",
    "            WL_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"Wavelength_PL\")):\n",
    "            WL_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_Abs\")):\n",
    "            DR_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_PL\")):\n",
    "            DR_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"LR\")):\n",
    "            LR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"FR\")):\n",
    "            FR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()    \n",
    "    return WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR\n",
    "def idx_min(y, x):\n",
    "    diff = np.abs(y[:, 0] - x)\n",
    "    idx = diff.argmin()\n",
    "    return idx\n",
    "# extract reactive phase\n",
    "def extract(x, idx1, idx2, numFirstElements):\n",
    "    x_mean = x[idx1:idx2 + 1, :].mean(axis = 0)\n",
    "    x_sort = np.sort(x_mean)\n",
    "    idx_phase = np.nonzero(np.in1d(x_mean, x_sort[:numFirstElements]))[0]\n",
    "    x_phase = x[:, idx_phase]\n",
    "    return x_phase\n",
    "def spectra_avg(x):\n",
    "    x_avg = x.mean(axis = 1) # avg over extracted reactive phase spectra for each WL; will result in a row matrix\n",
    "    x_avg = x_avg[:, np.newaxis]\n",
    "    return x_avg\n",
    "def baseline_zero(x, idx_low, idx_high):\n",
    "    x_baseline_zero = x - x[idx_low:idx_high + 1, :].mean() # make baseline zero; subtracting mean of PL at LL - HL nm\n",
    "    return x_baseline_zero\n",
    "def linear_int_x(y, x, i, y_btw):\n",
    "    x_btw = x[i, 0] - ((x[i + 1, 0] - x[i, 0]) / (y[i + 1, 0] - y[i, 0])) * (y[i, 0] - y_btw)\n",
    "    return x_btw\n",
    "def linear_int_y(y, x, i, x_btw):\n",
    "    y_btw = y[i, 0] - ((y[i + 1, 0] - y[i, 0]) / (x[i + 1, 0] - x[i, 0])) * (x[i, 0] - x_btw)\n",
    "    return y_btw\n",
    "def peak_info(y, x, emission_intensity):\n",
    "    # correction for emission peak intensity in case min is not exactly zero\n",
    "    # min_intensity = y.min()\n",
    "    min_intensity = 0 # without correction\n",
    "    intensity_peak = emission_intensity - min_intensity\n",
    "    # emission peak area\n",
    "    area_peak = np.trapz(y.T, x = x.T)[0]\n",
    "    # emission peak intensity and area\n",
    "    output = [intensity_peak, area_peak]\n",
    "    return np.array(output)[:, np.newaxis]\n",
    "# process ABS\n",
    "def spectra_extract_ABS(WL, DR, LR, ABS_all, ABS_names):\n",
    "    # baseline WL limits\n",
    "    baseline_LL = 700\n",
    "    baseline_HL = 800\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    lastPeak_LL = 250\n",
    "    lastPeak_HL = 350 \n",
    "    idx_lastPeak_LL = idx_min(WL, lastPeak_LL)\n",
    "    idx_lastPeak_HL = idx_min(WL, lastPeak_HL)\n",
    "    # excitation WL info\n",
    "    excitation_WL = 300\n",
    "    idx_excitationWL = idx_min(WL, excitation_WL)\n",
    "    # initiate files to be exported\n",
    "    WLABS_processed = WL\n",
    "    ABS_excitationWL_list = []\n",
    "    # ABS_excitationWL_list = np.array([[\"Absorbance at excitation WL\"]])\n",
    "    for (item, name) in zip(ABS_all, ABS_names):\n",
    "        ABS_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 50\n",
    "        ABS_phase = extract(ABS_only, idx_lastPeak_LL, idx_lastPeak_HL, param)\n",
    "        ABS_phase_avg = spectra_avg(ABS_phase) \n",
    "        arg_log = (ABS_phase_avg - DR) / (LR - DR) # Beer-Lambert law\n",
    "        ABS_processed = - np.log10(arg_log) # Beer-Lambert law\n",
    "        ABS_processed_baseline = baseline_zero(ABS_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLABS_processed = np.concatenate( [WLABS_processed, ABS_processed_baseline] , axis = 1) # attach WL and processed ABS\n",
    "        # WLABS_processed_filtered = WLABS_processed[np.isnan(WLABS_processed[:, 1]) == False] # remove NaN ABS\n",
    "        ABS_excitationWL = linear_int_y(ABS_processed_baseline, WL, idx_excitationWL, excitation_WL)\n",
    "        ABS_excitationWL_list.append(ABS_excitationWL)\n",
    "        # ABS_excitationWL_list = np.concatenate( [ABS_excitationWL_list, np.array([ABS_excitationWL])[:, np.newaxis]] , axis = 1)\n",
    "    ABS_excitationWL_list = np.array(ABS_excitationWL_list)[:, np.newaxis]\n",
    "    # return WLABS_processed, ABS_excitationWL_list # processed ABS spectra as well as the Abs300nm as one one of the optical features of interest\n",
    "    return ABS_excitationWL_list\n",
    "# process PL\n",
    "def spectra_extract_PL(WL, DR, PL_all, PL_names):\n",
    "    ## baseline WL limits\n",
    "    baseline_LL = 800\n",
    "    baseline_HL = 1000\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    excWL_LL = 250\n",
    "    excWL_HL = 350\n",
    "    idx_excWL_LL = idx_min(WL, excWL_LL)\n",
    "    idx_excWL_HL = idx_min(WL, excWL_HL)\n",
    "    ## emission peak WL and the WL range for the right side of emission peak\n",
    "    emPeak_WL = 446\n",
    "    emPeak_LL = 345\n",
    "    emPeak_HL = 700\n",
    "    idx_emPeak_LL = idx_min(WL, emPeak_LL)\n",
    "    idx_emPeak_HL = idx_min(WL, emPeak_HL)\n",
    "    WL_aroundPeak = WL[idx_emPeak_LL: idx_emPeak_HL + 1, :]\n",
    "    idx_intensityWL = idx_min(WL, emPeak_WL) # index for emission peak WL\n",
    "    ## initiate files to be exported\n",
    "    WLPL_processed = WL\n",
    "    emission_details_list = []\n",
    "    # emission_details = np.array([[\"Emission Peak WL\"], [\"Emission Peak Intensity\"], [\"FWHM\"], [\"Emission Peak Area\"]])\n",
    "    for (item, name) in zip(PL_all, PL_names):\n",
    "        PL_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 50\n",
    "        PL_phase = extract(PL_only, idx_excWL_LL, idx_excWL_HL, param) \n",
    "        PL_phase_avg = spectra_avg(PL_phase)\n",
    "        PL_processed = PL_phase_avg - DR\n",
    "        PL_processed_baseline = baseline_zero(PL_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLPL_processed = np.concatenate( [WLPL_processed, PL_processed_baseline] , axis = 1) # attach WL and processed PL\n",
    "        ## emission peak info\n",
    "        PL_emissionWL = linear_int_y(PL_processed_baseline, WL, idx_intensityWL, emPeak_WL)\n",
    "        PL_aroundPeak = PL_processed_baseline[idx_emPeak_LL: idx_emPeak_HL + 1, :]\n",
    "        info_peak = peak_info(PL_aroundPeak, WL_aroundPeak, PL_emissionWL)\n",
    "        emission_details_list.append(info_peak)\n",
    "        # emission_details = np.concatenate( [emission_details, info_peak_method] , axis = 1)\n",
    "    emission_details_list = np.concatenate(emission_details_list, axis = 1).T\n",
    "    # return WLPL_processed, emission_details_list # processed PL spectra as well as PLI and PLA as two of the optical features of interest\n",
    "    # return emission_details_list # emission peak intensity and area as optical features of interest\n",
    "    return emission_details_list[:, [1]] # emission peak area as one of the output parameters of interest to calculate relative PLQY\n",
    "## calculate relative PLQY\n",
    "def rel_PLQY(y_abs, y_area):\n",
    "    # Convert y_abs and y_area to pytorch tensors if they are numpy arrays\n",
    "    if not torch.is_tensor(y_abs):\n",
    "        y_abs = torch.from_numpy(y_abs.astype(np.float32))\n",
    "    if not torch.is_tensor(y_area):\n",
    "        y_area = torch.from_numpy(y_area.astype(np.float32))\n",
    "    # data related to the dye and solvent\n",
    "    PLQY_ref = 0.1336\n",
    "    ABS_ref = 0.9115\n",
    "    area_ref = 198493.1521\n",
    "    # calculate relative_PLQY\n",
    "    PLQY_sample = PLQY_ref * (y_area / area_ref) * ((1 - torch.pow(torch.tensor(10), -ABS_ref)) / (1 - torch.pow(torch.tensor(10), -y_abs)))\n",
    "    return PLQY_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fe09b",
   "metadata": {},
   "source": [
    "### ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale output\n",
    "def non_dim_y(y_initial):\n",
    "    # Convert y_initial to pytorch tensor if it isn't\n",
    "    if not torch.is_tensor(y_initial):\n",
    "        y_initial = torch.from_numpy(y_initial.astype(np.float32))\n",
    "    # Get min and max info\n",
    "    min_info, min_idx = torch.min(y_initial, axis=0, keepdim=True)\n",
    "    max_info, max_idx = torch.max(y_initial, axis=0, keepdim=True)\n",
    "    statNorm_info = torch.concatenate((min_info, max_info), axis=0)\n",
    "    # Nondimensionalize the y's\n",
    "    y_scaled = (y_initial - min_info) / (max_info - min_info)\n",
    "    return y_scaled, statNorm_info\n",
    "def dim_y(y_transform, statNorm_info):\n",
    "    # Get min and max info\n",
    "    min_info = torch.index_select(statNorm_info, dim=0, index=torch.tensor([0]))\n",
    "    max_info = torch.index_select(statNorm_info, dim=0, index=torch.tensor([1]))\n",
    "    # Give y's dimension\n",
    "    y_scaled = min_info + y_transform * (max_info - min_info)\n",
    "    return y_scaled\n",
    "## nondimensionalize inputs\n",
    "def non_dim_x(x_initial):\n",
    "    # For doping project\n",
    "    x_normalized = np.zeros((x_initial.shape[0], x_initial.shape[1] - 3))\n",
    "    x_normalized[:, 0] = (x_initial[:, 0] - 120) / (180 - 120) # x1 = temperature\n",
    "    x_normalized[:, 1] = (x_initial[:, 1] - 80) / (120 - 80) # x2 = CuI\n",
    "    x_normalized[:, 2] = (x_initial[:, 2] - 80) / (120 - 80) # x3 = ZnI2\n",
    "    x_normalized[:, 3] = (x_initial[:, 3] - 20) / (80 - 20) # x4 =  Cs-Oleate\n",
    "    x_normalized[:, 4] = (x_initial[:, 4] - 220) / (280 - 220) # x5 = ODE\n",
    "    return x_normalized\n",
    "## split dataset to training and validation\n",
    "def split_set(x, y, train_ratio = 0.8, seed_num = 4862): # important hyper-parameter\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = train_ratio, random_state = seed_num)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "## ENN structure\n",
    "def structure_info(num_models, layer_min = 8, layer_max = 10, node_min = 20, node_max = 40, seed_num = 100): # should tune these hyperparameters\n",
    "    random.seed(seed_num)\n",
    "    detail_all = []\n",
    "    for i in range (num_models):\n",
    "        num_layers = random.randint(layer_min, layer_max)\n",
    "        nodes_list = []\n",
    "        for j in range(num_layers):\n",
    "            num_nodes = random.randint(node_min, node_max)\n",
    "            nodes_list.append(num_nodes)\n",
    "        detail = [num_layers, nodes_list]\n",
    "        detail_all.append(detail)\n",
    "    return detail_all\n",
    "# Create Neural Network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, structure, network_type, input_shape, output_shape):\n",
    "        super(NeuralNetwork, self).__init__() # Initialize the network object using the superclass constructor\n",
    "        self.layers = nn.ModuleList() #List of layers\n",
    "        self.network_type = network_type\n",
    "        input_size = input_shape[-1] # Last dimension stores num of input features\n",
    "        # If network is a feed-forward neural network\n",
    "        if self.network_type == \"ff\":\n",
    "            for num_layers, nodes_list in structure:\n",
    "                for num_nodes in nodes_list:\n",
    "                    self.layers.append(nn.Linear(input_size, num_nodes)) # Apply the linear layer\n",
    "                    self.layers.append(nn.ReLU()) # Apply the ReLU activation\n",
    "                    input_size = num_nodes #reset input size to be the number of nodes determined by structure_info function\n",
    "            self.layers.append(nn.Linear(input_size, output_shape[-1]))  # final layer outputs predicted y's\n",
    "        # If network is a cascade neural network\n",
    "        elif self.network_type == \"cascade\":\n",
    "            for num_layers, nodes_list in structure:\n",
    "                for num_nodes in nodes_list:\n",
    "                    self.layers.append(nn.Linear(input_size, num_nodes)) # Apply the linear layer\n",
    "                    self.layers.append(nn.ReLU()) # Apply the ReLU activation\n",
    "                    input_size += num_nodes #reset input size to be the number of nodes determined by structure_info function\n",
    "            self.output_layer = nn.Linear(input_size, output_shape[-1]) # number of output parameters\n",
    "    def forward(self, x):\n",
    "        # If network is a feed-forward neural network\n",
    "        if self.network_type == \"ff\": \n",
    "            for layer in self.layers:\n",
    "                x = layer(x)          \n",
    "            return x\n",
    "        # If network is a cascade neural network\n",
    "        elif self.network_type == \"cascade\":\n",
    "            outputs = x\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    x = layer(outputs)  # Apply the linear layer\n",
    "                elif isinstance(layer, nn.ReLU):\n",
    "                    x = layer(x)  # Apply the ReLU activation\n",
    "                    outputs = torch.cat((outputs, x), dim = outputs.ndim-1) # Concatenate the current output to all previous ReLU outputs, along the last dimension\n",
    "            x = self.output_layer(outputs)     \n",
    "            return x\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.patience_counter = 0\n",
    "        self.best_model_state = None\n",
    "        self.min_validation_loss = float('inf')\n",
    "    def should_stop_early(self, model, current_validation_loss):\n",
    "        # Reset counter if new minimum validation loss found\n",
    "        if current_validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = current_validation_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.patience_counter = 0\n",
    "        # If validation loss is greater than minimum, increase the patience counter\n",
    "        elif current_validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    def get_best_model_state(self):\n",
    "        return self.best_model_state\n",
    "## Training the neural network using PyTorch\n",
    "def parallel_train_ML_pytorch(train_x, train_y, val_x, val_y, structure, network_type, batch_size=8, num_epochs=3000, learning_rate=1e-4, patience=500, min_delta=1e-4):\n",
    "    # Convert dataset to tensors if they are numpy arrays\n",
    "    if not torch.is_tensor(train_x):\n",
    "        train_x = torch.from_numpy(train_x.astype(np.float32))\n",
    "    if not torch.is_tensor(train_y):\n",
    "        train_y = torch.from_numpy(train_y.astype(np.float32))\n",
    "    if not torch.is_tensor(val_x):\n",
    "        val_x = torch.from_numpy(val_x.astype(np.float32))\n",
    "    if not torch.is_tensor(val_y):\n",
    "        val_y = torch.from_numpy(val_y.astype(np.float32))\n",
    "    # Instantiate training and test data\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(val_x, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(val_y), shuffle=False)\n",
    "    # Create model, optimizer, early stopping class, and loss function \n",
    "    model = NeuralNetwork(structure=structure, network_type=network_type, input_shape=train_x.shape, output_shape=train_y.shape)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #lr - learning step\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    criterion = nn.MSELoss()\n",
    "    # Data to store during training\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "    epochs = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    # Number of outputs to train\n",
    "    num_outputs = train_y.shape[-1]\n",
    "    # Train the NN\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Model Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epochs.append(epoch)\n",
    "        for inputs, labels in train_loader:\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Feed inputs\n",
    "            outputs = model(inputs)\n",
    "            # Loss function is the sum of output losses\n",
    "            loss = None\n",
    "            for train_output_idx in range(num_outputs):\n",
    "                train_output = torch.index_select(outputs, dim=outputs.ndim-1, index=torch.tensor(train_output_idx))\n",
    "                train_label = torch.index_select(labels, dim=labels.ndim-1, index=torch.tensor(train_output_idx))\n",
    "                if loss is None:\n",
    "                    loss = criterion(train_output, train_label)\n",
    "                else:\n",
    "                    loss += criterion(train_output, train_label)\n",
    "            # Back propagation\n",
    "            loss.backward()\n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "            # Add loss to accumulated loss \n",
    "            running_loss += loss.item()\n",
    "        # Compute average loss per batch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        ## Model Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            # Validate the NN\n",
    "            for val_inputs, val_labels in test_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = None\n",
    "                # Validation loss is the sum of output losses\n",
    "                for val_output_idx in range(num_outputs):\n",
    "                    val_output = torch.index_select(val_outputs, dim=val_outputs.ndim-1, index=torch.tensor(val_output_idx))\n",
    "                    val_label = torch.index_select(val_labels, dim=val_labels.ndim-1, index=torch.tensor(val_output_idx))\n",
    "                    if val_loss is None:\n",
    "                        val_loss = criterion(val_output, val_label).item()\n",
    "                    else:\n",
    "                        val_loss += criterion(val_output, val_label).item()\n",
    "            # Compute average loss per batch\n",
    "            val_loss /= len(test_loader)\n",
    "            val_loss_list.append(val_loss)\n",
    "        #print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        if early_stopping.should_stop_early(model, val_loss):\n",
    "            #print(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            break\n",
    "    # Load the best model\n",
    "    if early_stopping.get_best_model_state() is not None:\n",
    "        model.load_state_dict(early_stopping.get_best_model_state())\n",
    "    # Gather history\n",
    "    history['epoch'] = epochs\n",
    "    history['train_loss'] = train_loss_list\n",
    "    history['val_loss'] = val_loss_list\n",
    "    result = [model, history]\n",
    "    return result\n",
    "## deconvolute the model and training history\n",
    "def deconvolute_ML(ensemble_result):\n",
    "    ensemble_model = []\n",
    "    ensemble_history = []\n",
    "    for element in ensemble_result:\n",
    "        ensemble_model.append(element[0])\n",
    "        ensemble_history.append(element[1])\n",
    "    return ensemble_model, ensemble_history\n",
    "## plots related to the training process\n",
    "def learn_plot(history):\n",
    "    fig = plt.figure()\n",
    "    # loss values - MSE\n",
    "    plt.plot(history['train_loss'], color = 'blue')\n",
    "    plt.plot(history['val_loss'], color = 'red')\n",
    "    plt.title('Loss (MSE) - Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "## Plot for predicted vs. true outputs - for individual models\n",
    "def predict_plot_separate(model, x, y_true):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    dim = y_true.shape[-1]\n",
    "    plt.figure(figsize = (3.5 * dim, 3.5), dpi = 200)\n",
    "    # Plot each output separately\n",
    "    for i in range(dim):\n",
    "        y_true_i = y_true[:,i]\n",
    "        model_prediction = model(x).detach().numpy()\n",
    "        y_predicted_i = model_prediction[:, i]\n",
    "        detail = linregress(y_true_i, y_predicted_i)\n",
    "        plt.subplot(1, dim, i + 1)\n",
    "        plt.scatter(y_true_i, y_predicted_i, color = 'blue', alpha = 0.8, label = 'y' + str(i) + ': $R^2 = $' + str(round((detail.rvalue ** 2), 4)))\n",
    "        plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "        # plt.title('y' + str(i + 1))\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.legend() \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for the entire ENN \n",
    "def predict_plot_mean(ensemble_NN, x, y_true):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    y_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        y_pred.append(model(x).detach().numpy())\n",
    "    y_pred_arr = np.concatenate(y_pred, axis = 1)\n",
    "    dim = y_true.shape[1]\n",
    "    y_pred_mean = []\n",
    "    y_pred_std = []\n",
    "    plt.figure(figsize = (3.5 * dim, 3.5), dpi = 200)\n",
    "    # Plot each output separately\n",
    "    for i in range(dim):\n",
    "        element = y_pred_arr[:, i::dim]\n",
    "        element_mean = np.mean(element, axis = 1)\n",
    "        element_std = np.std(element, axis = 1)\n",
    "        element_detail = linregress(y_true[:, i], element_mean)\n",
    "        y_pred_mean.append(element_mean[:, np.newaxis])\n",
    "        y_pred_std.append(element_std[:, np.newaxis])\n",
    "        plt.subplot(1, dim, i + 1)\n",
    "        plt.errorbar(y_true[:, i], element_mean, yerr = element_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = 'y' + str(i) + ': $R^2 = $' + str(round((element_detail.rvalue ** 2), 4)))\n",
    "        plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "        # plt.title('y' + str(i + 1))\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    pred_mean = np.concatenate(y_pred_mean, axis = 1)\n",
    "    pred_std = np.concatenate(y_pred_std, axis = 1)\n",
    "    return pred_mean, pred_std\n",
    "# Predict relative PLQY and compare it to true relative PLQY\n",
    "def predict_plot_mean_PLQY(ensemble_NN, x, y_true, statNorm_info):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    rel_PLQY_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        predictions = model(x).detach()\n",
    "        y_pred_dim = dim_y(predictions, statNorm_info)\n",
    "        rel_PLQY_pred.append(rel_PLQY(y_pred_dim[:,[0]], y_pred_dim[:,[1]]).numpy())\n",
    "    rel_PLQY_pred_arr = np.concatenate(rel_PLQY_pred, axis = 1)\n",
    "    plt.figure(figsize = (3.5, 3.5), dpi = 200)\n",
    "    rel_PLQY_pred_mean = np.mean(rel_PLQY_pred_arr, axis = 1)\n",
    "    rel_PLQY_pred_std = np.std(rel_PLQY_pred_arr, axis = 1)\n",
    "    y_true_dim = dim_y(y_true, statNorm_info)\n",
    "    rel_PLQY_true = rel_PLQY(y_true_dim[:,0], y_true_dim[:,1]).numpy()\n",
    "    element_detail = linregress(rel_PLQY_true, rel_PLQY_pred_mean)\n",
    "    plt.errorbar(rel_PLQY_true, rel_PLQY_pred_mean, yerr = rel_PLQY_pred_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = '$R^2 = $' + str(round((element_detail.rvalue ** 2), 4)))\n",
    "    plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "    plt.title('Relative PLQY')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('True')\n",
    "    plt.xlim([0.0, 0.25])\n",
    "    plt.ylim([0.0, 0.25])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    return rel_PLQY_pred_mean, rel_PLQY_pred_std\n",
    "## save the ENN that's been trained\n",
    "def save_ENN(ENN, path):\n",
    "    for n, model in enumerate(ENN):\n",
    "        model_path = os.path.join(path, str(n))\n",
    "        os.mkdir(model_path)\n",
    "        torch.save(model, os.path.join(model_path , \"model\" + str(n)))\n",
    "## load the ENN that's been saved\n",
    "def load_ENN(path):\n",
    "    model_idx = sorted(os.listdir(path), key = len)\n",
    "    ensemble = []\n",
    "    for idx in model_idx:\n",
    "        model_path = os.path.join(path, idx)\n",
    "        ensemble.append(torch.load(os.path.join(model_path, \"model\" + idx)))\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f8368",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c42f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During optimization, this function is necessary to handle the case where the model output tensor has three dimensions, i.e. (len(X), 1, 2)\n",
    "def get_relative_PLQY(y):\n",
    "    # calculate relative_PLQY    \n",
    "    y_abs = torch.index_select(y, dim=y.ndim-1, index=torch.tensor([0]))\n",
    "    y_area = torch.index_select(y, dim=y.ndim-1, index=torch.tensor([1]))\n",
    "    PLQY_sample = rel_PLQY(y_abs, y_area)\n",
    "    return PLQY_sample\n",
    "class ENN_Posterior(Posterior):\n",
    "    # Abstract property device\n",
    "    device = torch.device('cpu')\n",
    "    # Abstract property dtype\n",
    "    dtype = torch.float32\n",
    "    # Abstract property event_shape (the shape of a single sample)\n",
    "    event_shape = torch.Size()\n",
    "    def __init__(self, X, ensemble_py, statNorm_info, optimize_options):\n",
    "        self.X = X\n",
    "        self.ensemble_py = ensemble_py\n",
    "        # Get the predictions for each model in the ensemble\n",
    "        outputs = torch.tensor([])\n",
    "        for model in ensemble_py:\n",
    "            y_nondim = model(X)\n",
    "            y_dim = dim_y(y_nondim, statNorm_info) # Dimensionalize the outputs\n",
    "            y_pred = get_relative_PLQY(y_dim) # Calculate relative PLQY\n",
    "            if (optimize_options[0] == \"minimize\"):\n",
    "                y_target = optimize_options[1]\n",
    "                obj_fun = torch.abs(y_target - y_pred) \n",
    "            else:\n",
    "                obj_fun = y_pred\n",
    "            # Concatenate the objective function to the outputs tensor\n",
    "            outputs = torch.cat((outputs, obj_fun), dim=1)\n",
    "        # Calculate and store the mean of ensemble predictions\n",
    "        torch_mean = torch.mean(outputs, dim=1)\n",
    "        self.mean = torch_mean\n",
    "        # Calculate and store the variance of ensemble predictions\n",
    "        torch_var = torch.var(outputs, dim=1)\n",
    "        self.variance = torch_var\n",
    "    def rsample(self, sample_shape=None, base_samples=None):\n",
    "        # Construct a normal distribution and sample from it\n",
    "        mu = self.mean\n",
    "        sigma = self.variance\n",
    "        dist = torch.normal(mu, sigma) \n",
    "        return dist\n",
    "class ENN_Model(Model):\n",
    "    num_outputs=None #need to have num_outputs defined or else error is thrown\n",
    "    def __init__(self, ensemble_py, statNorm_info, optimize_options, num_objectives, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ensemble_py = ensemble_py\n",
    "        self.statNorm_info = statNorm_info\n",
    "        self.optimize_options = optimize_options\n",
    "        self.num_outputs = num_objectives\n",
    "    # Computes the posterior over the model output at the provided points.\n",
    "    def posterior(self, X, output_indices=None, observation_noise=False, posterior_transform=None, **kwargs):\n",
    "        return ENN_Posterior(X, self.ensemble_py, self.statNorm_info, self.optimize_options)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used of expected improvement (EI)\n",
    "def best_data(y_abs, y_area, optimize_options):\n",
    "    obj_fun_data = rel_PLQY(y_abs, y_area)\n",
    "    if optimize_options[0] == \"maximize\":\n",
    "        obj_fun_best = obj_fun_data.max()\n",
    "    elif optimize_options[0] == \"minimize\":\n",
    "        y_target = optimize_options[1]\n",
    "        obj_fun_best = torch.abs(y_target - obj_fun_data).min()\n",
    "    return obj_fun_best\n",
    "## aquisition functions\n",
    "def botorch_optimize(ensemble, optimize_options, policy, obj_fun_best, input_dim, statNorm_info, num_objectives=1, num_restarts=100, num_samples=500000):\n",
    "    trained_model = ENN_Model(ensemble, statNorm_info, optimize_options, num_objectives)\n",
    "    maximize = (True if optimize_options[0] == \"maximize\" else False)\n",
    "    if policy == \"EI\":\n",
    "        acqf = LogExpectedImprovement(trained_model, best_f=obj_fun_best, maximize=maximize)\n",
    "    elif policy == \"PI\":\n",
    "        acqf = LogProbabilityOfImprovement(trained_model, best_f=obj_fun_best, maximize=maximize)\n",
    "    elif policy == \"EPLT\":\n",
    "        acqf = PosteriorMean(trained_model, maximize=maximize)\n",
    "    else:\n",
    "        print('The policy type is invalid!')\n",
    "        return None, None\n",
    "    bounds = torch.tensor([[0. for i in range(input_dim)], [1. for i in range(input_dim)]])\n",
    "    x_best, best_acq_value = optimize_acqf(acqf, bounds, q=1, num_restarts=num_restarts, raw_samples=num_samples, gen_candidates=gen_candidates_torch, return_best_only=True)\n",
    "    y_best = trained_model.posterior(x_best).mean # The ensemble mean is the prediction\n",
    "    x_best = x_best.detach().numpy() # Remove the gradient info and convert to numpy\n",
    "    y_best = y_best.detach().numpy() # Remove the gradient info and convert to numpy\n",
    "    return x_best, y_best\n",
    "## optimization\n",
    "def exp_sel_botorch(y, FR, optimize_options, policy, network_type=\"cascade\", batch_size = 8, num_epochs=3000, learning_rate=1e-4, patience=500, min_delta=1e-4, num_models=20, num_objectives=1, num_restarts=100, num_samples=500000):\n",
    "    ## dataset and preprocessing needed to be able to train the model\n",
    "    y_norm, statNorm_info = non_dim_y(y)\n",
    "    # Get best objective function seen for optimization\n",
    "    obj_fun_best = best_data(y[:, [0]], y[:,[1]], optimize_options)\n",
    "    x_norm = non_dim_x(FR)\n",
    "    x_train, x_valid, y_train, y_valid = split_set(x_norm, y_norm)\n",
    "    ## train the ENN\n",
    "    start_ML = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    architecture = structure_info(num_models)\n",
    "    result_aggregate_ML = Parallel(n_jobs = -2)(delayed(parallel_train_ML_pytorch)(x_train, y_train, x_valid, y_valid, [architecture[i]], network_type=network_type, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, patience=patience, min_delta=min_delta) for i in range(num_models))\n",
    "    ensemble, history = deconvolute_ML(result_aggregate_ML)\n",
    "    end_ML = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time to train the ensemble NN is {str(end_ML - start_ML)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    ## optimization and achive the best input variables needed for the future experiments\n",
    "    start_opt = time()\n",
    "    input_dim = x_norm.shape[-1]\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    x_best, y_best = botorch_optimize(ensemble, optimize_options, policy, obj_fun_best, input_dim, statNorm_info, num_objectives=num_objectives, num_restarts=num_restarts, num_samples=num_samples)    \n",
    "    end_opt = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time for optimization is {str(end_opt - start_opt)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    return x_best, y_best, ensemble, history, x_train, y_train, x_valid, y_valid, architecture, obj_fun_best, statNorm_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b50df",
   "metadata": {},
   "source": [
    "### Master Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path):\n",
    "    files_all = os.listdir(path)\n",
    "    count_ABS = 0\n",
    "    count_PL = 0\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            count_ABS = count_ABS + 1\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            count_PL = count_PL + 1\n",
    "    return count_ABS, count_PL\n",
    "def x_dim(x_norm, FR, path):\n",
    "    ## for doping project; should be updated later\n",
    "    x_final = np.zeros((x_norm.shape[0], x_norm.shape[1] + 3))\n",
    "    x_final[:, 0] = (x_norm[:, 0] * (180 - 120)) + 120\n",
    "    x_final[:, 1] = (x_norm[:, 1] * (120 - 80)) + 80\n",
    "    x_final[:, 2] = (x_norm[:, 2] * (120 - 80)) + 80\n",
    "    x_final[:, 3] = (x_norm[:, 3] * (80 - 20)) + 20\n",
    "    x_final[:, 4] = (x_norm[:, 4] * (280 - 220)) + 220\n",
    "    # last three columns\n",
    "    x_final[:, -3] = 0.5 * np.sum(x_final[:, 1:5], axis = 1) # x6 = PFO\n",
    "    x_final[:, -2] = 2 # x7 = 2 always; wait for 2 residence times then record the spectra\n",
    "    x_final[:, -1] = 0 # x8 = 0 always; no washing since we have PFO as carrier phase\n",
    "    updated_FR = np.concatenate((FR, x_final), axis = 0) ## concat previous and new input parameters\n",
    "    np.savetxt(path + \"//\" + \"FR.csv\", updated_FR, delimiter = \",\", fmt=\"%1.2f\") # overwrite it into the existing FR.csv file\n",
    "    return x_final, updated_FR\n",
    "## master function\n",
    "# optimize_options = [\"maximize\", 0] or [\"minimize\", y_target]\n",
    "# policy = \"EI\" or \"PI\" or \"EPLT\"\n",
    "# def master_code(exp_budget = 75, optimize_options = [\"maximize\", 0.0], policy = \"EI\", network_type=\"cascade\", batch_size=8, num_epochs=3000, learning_rate=1e-4, patience=500, min_delta=1e-4, num_models=20, num_objectives=1, num_restarts=100, num_samples=500000, num_selection = 1): # if EI is used\n",
    "def master_code(exp_budget = 75, optimize_options = [\"maximize\", 0.0], policy = \"EI\", network_type=\"cascade\", batch_size=16, num_epochs=3000, learning_rate=1e-3, patience=100, min_delta=1e-4, num_models=20, num_objectives=1, num_restarts=100, num_samples=500000, num_selection = 1): # if EI is used\n",
    "    loc = input(\"Please enter the path for the directory that includes files: \")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    num_files_old = 0\n",
    "    while True:\n",
    "        num_ABS, num_PL = count_files(loc)\n",
    "        if (num_ABS == num_PL):\n",
    "            num_files = num_PL\n",
    "        policy = (policy if num_files < exp_budget else \"EPLT\")\n",
    "        if (num_files > num_files_old + num_selection - 1) and (num_files <= exp_budget):\n",
    "            print(\"New run starts now:\")\n",
    "            num_files_old = num_files\n",
    "            WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR = read_files(loc)\n",
    "            y_abs = spectra_extract_ABS(WL_ABS, DR_ABS, LR, ABS_all, ABS_names)\n",
    "            y_area = spectra_extract_PL(WL_PL, DR_PL, PL_all, PL_names)\n",
    "            y = np.concatenate((y_abs, y_area), axis = 1)\n",
    "            new_x, new_y, ensemble, history, x_train, y_train, x_valid, y_valid, structure, obj_fun_best, statNorm_info = exp_sel_botorch(y, FR, optimize_options, policy, network_type=network_type, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, patience=patience, min_delta=min_delta, num_models=num_models, num_objectives=num_objectives, num_restarts=num_restarts, num_samples=num_samples)\n",
    "            new_FR, updated_FR = x_dim(new_x, FR, loc)\n",
    "        elif (num_files > exp_budget):\n",
    "            print(\"The experimental budget is reached!\")\n",
    "            break\n",
    "#         # use this for the test run using the provided dataset \n",
    "#         if (num_files_old == 60):\n",
    "#             break\n",
    "#     return new_x, new_y, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR, obj_fun_best, statNorm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85383c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_x, new_y, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR, obj_fun_best, statNorm_info = master_code(exp_budget = 70, optimize_options = [\"maximize\", 0.0], policy = \"EI\", network_type=\"cascade\", batch_size=8, num_epochs=3000, learning_rate=1e-4, patience=500, min_delta=1e-6, num_models=20, num_objectives=1, num_restarts=100, num_samples=500000, num_selection = 1)\n",
    "master_code()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
